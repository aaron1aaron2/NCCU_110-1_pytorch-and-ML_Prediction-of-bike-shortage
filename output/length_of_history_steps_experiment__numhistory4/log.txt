K=8, L=1, SE_file='data/train_data/SE/basic/SE.txt', batch_size=24, d=8, decay_epoch=10, device='cuda', learning_rate=0.01, log_file='./output/length_of_history_steps_experiment__numhistory4/log.txt', max_epoch=50, model_file='./output/length_of_history_steps_experiment__numhistory4/model.pkl', num_his=4, num_pred=1, output_folder='./output/length_of_history_steps_experiment__numhistory4', patience=100, test_ratio=0.2, time_slot=10, traffic_file='data/train_data/data.h5', train_ratio=0.7, val_ratio=0.1, view_batch_freq=100
main output folder./output/length_of_history_steps_experiment__numhistory4
loading data...
trainX: torch.Size([3623, 4, 26])		 trainY: torch.Size([3623, 1, 26])
valX:   torch.Size([514, 4, 26])		valY:   torch.Size([514, 1, 26])
testX:   torch.Size([1032, 4, 26])		testY:   torch.Size([1032, 1, 26])
mean:   11.0475		std:   6.8504
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-01-11 09:50:57 | epoch: 0001/50, training time: 9.1s, inference time: 0.4s
train loss: 3.5736, val_loss: 2.5907
val loss decrease from inf to 2.5907, saving model to ./output/length_of_history_steps_experiment__numhistory4/model.pkl
2022-01-11 09:51:07 | epoch: 0002/50, training time: 9.2s, inference time: 0.4s
train loss: 2.2254, val_loss: 2.3420
val loss decrease from 2.5907 to 2.3420, saving model to ./output/length_of_history_steps_experiment__numhistory4/model.pkl
2022-01-11 09:51:17 | epoch: 0003/50, training time: 9.3s, inference time: 0.4s
train loss: 2.0990, val_loss: 2.3136
val loss decrease from 2.3420 to 2.3136, saving model to ./output/length_of_history_steps_experiment__numhistory4/model.pkl
2022-01-11 09:51:26 | epoch: 0004/50, training time: 9.2s, inference time: 0.4s
train loss: 1.9885, val_loss: 2.3616
2022-01-11 09:51:36 | epoch: 0005/50, training time: 9.2s, inference time: 0.4s
train loss: 1.9898, val_loss: 2.6013
2022-01-11 09:51:46 | epoch: 0006/50, training time: 9.2s, inference time: 0.4s
train loss: 1.9357, val_loss: 2.2674
val loss decrease from 2.3136 to 2.2674, saving model to ./output/length_of_history_steps_experiment__numhistory4/model.pkl
2022-01-11 09:51:55 | epoch: 0007/50, training time: 9.1s, inference time: 0.4s
train loss: 1.9502, val_loss: 2.2635
val loss decrease from 2.2674 to 2.2635, saving model to ./output/length_of_history_steps_experiment__numhistory4/model.pkl
2022-01-11 09:52:05 | epoch: 0008/50, training time: 9.2s, inference time: 0.4s
train loss: 1.8992, val_loss: 2.2296
val loss decrease from 2.2635 to 2.2296, saving model to ./output/length_of_history_steps_experiment__numhistory4/model.pkl
2022-01-11 09:52:14 | epoch: 0009/50, training time: 9.2s, inference time: 0.4s
train loss: 1.9105, val_loss: 2.6943
2022-01-11 09:52:24 | epoch: 0010/50, training time: 9.3s, inference time: 0.4s
train loss: 1.9136, val_loss: 2.2514
2022-01-11 09:52:34 | epoch: 0011/50, training time: 9.2s, inference time: 0.4s
train loss: 1.8763, val_loss: 2.2218
val loss decrease from 2.2296 to 2.2218, saving model to ./output/length_of_history_steps_experiment__numhistory4/model.pkl
2022-01-11 09:52:43 | epoch: 0012/50, training time: 9.2s, inference time: 0.4s
train loss: 1.8416, val_loss: 2.1158
val loss decrease from 2.2218 to 2.1158, saving model to ./output/length_of_history_steps_experiment__numhistory4/model.pkl
2022-01-11 09:52:53 | epoch: 0013/50, training time: 9.2s, inference time: 0.4s
train loss: 1.8320, val_loss: 2.2176
2022-01-11 09:53:02 | epoch: 0014/50, training time: 9.2s, inference time: 0.4s
train loss: 1.8011, val_loss: 2.4527
2022-01-11 09:53:12 | epoch: 0015/50, training time: 9.3s, inference time: 0.4s
train loss: 1.8283, val_loss: 2.1708
2022-01-11 09:53:22 | epoch: 0016/50, training time: 9.1s, inference time: 0.4s
train loss: 1.8227, val_loss: 2.4585
2022-01-11 09:53:31 | epoch: 0017/50, training time: 9.2s, inference time: 0.4s
train loss: 1.8098, val_loss: 2.2451
2022-01-11 09:53:41 | epoch: 0018/50, training time: 9.4s, inference time: 0.4s
train loss: 1.8045, val_loss: 2.2320
2022-01-11 09:53:51 | epoch: 0019/50, training time: 9.4s, inference time: 0.4s
train loss: 1.8040, val_loss: 2.3362
2022-01-11 09:54:01 | epoch: 0020/50, training time: 9.3s, inference time: 0.4s
train loss: 1.8080, val_loss: 2.2254
2022-01-11 09:54:10 | epoch: 0021/50, training time: 9.2s, inference time: 0.4s
train loss: 1.7541, val_loss: 2.1661
2022-01-11 09:54:20 | epoch: 0022/50, training time: 9.2s, inference time: 0.4s
train loss: 1.7564, val_loss: 2.1598
2022-01-11 09:54:29 | epoch: 0023/50, training time: 9.2s, inference time: 0.4s
train loss: 1.7268, val_loss: 2.1562
2022-01-11 09:54:39 | epoch: 0024/50, training time: 9.2s, inference time: 0.4s
train loss: 1.7152, val_loss: 2.1748
2022-01-11 09:54:49 | epoch: 0025/50, training time: 9.3s, inference time: 0.4s
train loss: 1.7105, val_loss: 2.1433
2022-01-11 09:54:58 | epoch: 0026/50, training time: 9.2s, inference time: 0.4s
train loss: 1.7195, val_loss: 2.3396
2022-01-11 09:55:08 | epoch: 0027/50, training time: 9.1s, inference time: 0.4s
train loss: 1.6893, val_loss: 2.1368
2022-01-11 09:55:18 | epoch: 0028/50, training time: 9.2s, inference time: 0.4s
train loss: 1.7019, val_loss: 2.1857
2022-01-11 09:55:27 | epoch: 0029/50, training time: 9.3s, inference time: 0.4s
train loss: 1.6630, val_loss: 2.2254
2022-01-11 09:55:37 | epoch: 0030/50, training time: 9.4s, inference time: 0.4s
train loss: 1.6934, val_loss: 2.2424
2022-01-11 09:55:47 | epoch: 0031/50, training time: 9.2s, inference time: 0.4s
train loss: 1.6866, val_loss: 2.3507
2022-01-11 09:55:56 | epoch: 0032/50, training time: 9.3s, inference time: 0.4s
train loss: 1.6395, val_loss: 2.3131
2022-01-11 09:56:06 | epoch: 0033/50, training time: 9.3s, inference time: 0.4s
train loss: 1.6110, val_loss: 2.2814
2022-01-11 09:56:15 | epoch: 0034/50, training time: 9.2s, inference time: 0.4s
train loss: 1.6060, val_loss: 2.2663
2022-01-11 09:56:25 | epoch: 0035/50, training time: 9.2s, inference time: 0.4s
train loss: 1.5848, val_loss: 2.4000
2022-01-11 09:56:35 | epoch: 0036/50, training time: 9.4s, inference time: 0.4s
train loss: 1.6428, val_loss: 2.2518
2022-01-11 09:56:45 | epoch: 0037/50, training time: 9.4s, inference time: 0.4s
train loss: 1.6040, val_loss: 2.2295
2022-01-11 09:56:54 | epoch: 0038/50, training time: 9.2s, inference time: 0.4s
train loss: 1.5819, val_loss: 2.2719
2022-01-11 09:57:04 | epoch: 0039/50, training time: 9.3s, inference time: 0.4s
train loss: 1.5843, val_loss: 2.6555
2022-01-11 09:57:14 | epoch: 0040/50, training time: 9.3s, inference time: 0.4s
train loss: 1.5390, val_loss: 2.4270
2022-01-11 09:57:23 | epoch: 0041/50, training time: 9.2s, inference time: 0.4s
train loss: 1.5211, val_loss: 2.2868
2022-01-11 09:57:33 | epoch: 0042/50, training time: 9.1s, inference time: 0.4s
train loss: 1.5129, val_loss: 2.2605
2022-01-11 09:57:42 | epoch: 0043/50, training time: 9.3s, inference time: 0.4s
train loss: 1.5154, val_loss: 2.2465
2022-01-11 09:57:52 | epoch: 0044/50, training time: 9.2s, inference time: 0.4s
train loss: 1.4824, val_loss: 2.5734
2022-01-11 09:58:02 | epoch: 0045/50, training time: 9.2s, inference time: 0.4s
train loss: 1.5047, val_loss: 2.2930
2022-01-11 09:58:11 | epoch: 0046/50, training time: 9.1s, inference time: 0.4s
train loss: 1.4602, val_loss: 2.3209
2022-01-11 09:58:21 | epoch: 0047/50, training time: 9.2s, inference time: 0.4s
train loss: 1.4325, val_loss: 2.6719
2022-01-11 09:58:30 | epoch: 0048/50, training time: 9.2s, inference time: 0.4s
train loss: 1.4532, val_loss: 2.4237
2022-01-11 09:58:40 | epoch: 0049/50, training time: 9.2s, inference time: 0.4s
train loss: 1.4599, val_loss: 2.5174
2022-01-11 09:58:50 | epoch: 0050/50, training time: 9.2s, inference time: 0.4s
train loss: 1.4014, val_loss: 2.3571
Training and validation are completed, and model has been stored as ./output/length_of_history_steps_experiment__numhistory4/model.pkl
**** testing model ****
loading model from ./output/length_of_history_steps_experiment__numhistory4/model.pkl
model restored!
evaluating...
testing time: 0.8s
                MAE		RMSE		MAPE
train            0.64		1.10		10.27%
val              0.91		1.56		14.01%
test             0.80		1.45		12.64%
performance in each prediction step
step: 01         0.80		1.45		12.64%
average:         0.80		1.45		12.64%
total time: 8.1min
