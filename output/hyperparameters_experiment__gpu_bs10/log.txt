K=8, L=1, SE_file='data/train_data/SE/basic/SE.txt', batch_size=10, d=8, decay_epoch=10, device='cuda', learning_rate=0.001, log_file='./output/hyperparameters_experiment__gpu_bs10/log.txt', max_epoch=50, model_file='./output/hyperparameters_experiment__gpu_bs10/model.pkl', num_his=5, num_pred=1, output_folder='./output/hyperparameters_experiment__gpu_bs10', patience=100, test_ratio=0.2, time_slot=10, traffic_file='data/train_data/data.h5', train_ratio=0.7, val_ratio=0.1, view_batch_freq=100
main output folder./output/hyperparameters_experiment__gpu_bs10
loading data...
trainX: torch.Size([3622, 5, 26])		 trainY: torch.Size([3622, 1, 26])
valX:   torch.Size([513, 5, 26])		valY:   torch.Size([513, 1, 26])
testX:   torch.Size([1031, 5, 26])		testY:   torch.Size([1031, 1, 26])
mean:   11.0472		std:   6.8502
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-01-10 06:48:52 | epoch: 0001/50, training time: 18.8s, inference time: 0.8s
train loss: 4.9647, val_loss: 3.9747
val loss decrease from inf to 3.9747, saving model to ./output/hyperparameters_experiment__gpu_bs10/model.pkl
2022-01-10 06:49:11 | epoch: 0002/50, training time: 18.6s, inference time: 0.8s
train loss: 3.4968, val_loss: 3.8496
val loss decrease from 3.9747 to 3.8496, saving model to ./output/hyperparameters_experiment__gpu_bs10/model.pkl
2022-01-10 06:49:31 | epoch: 0003/50, training time: 18.6s, inference time: 0.8s
train loss: 3.3933, val_loss: 3.7569
val loss decrease from 3.8496 to 3.7569, saving model to ./output/hyperparameters_experiment__gpu_bs10/model.pkl
2022-01-10 06:49:50 | epoch: 0004/50, training time: 18.7s, inference time: 0.8s
train loss: 3.2656, val_loss: 3.6586
val loss decrease from 3.7569 to 3.6586, saving model to ./output/hyperparameters_experiment__gpu_bs10/model.pkl
2022-01-10 06:50:10 | epoch: 0005/50, training time: 18.6s, inference time: 0.8s
train loss: 3.1698, val_loss: 3.5139
val loss decrease from 3.6586 to 3.5139, saving model to ./output/hyperparameters_experiment__gpu_bs10/model.pkl
2022-01-10 06:50:29 | epoch: 0006/50, training time: 18.6s, inference time: 0.8s
train loss: 3.1130, val_loss: 3.5841
2022-01-10 06:50:48 | epoch: 0007/50, training time: 18.4s, inference time: 0.8s
train loss: 3.0511, val_loss: 3.5196
2022-01-10 06:51:07 | epoch: 0008/50, training time: 18.3s, inference time: 0.8s
train loss: 3.0337, val_loss: 3.5700
2022-01-10 06:51:26 | epoch: 0009/50, training time: 18.3s, inference time: 0.8s
train loss: 2.9767, val_loss: 3.5621
2022-01-10 06:51:46 | epoch: 0010/50, training time: 18.3s, inference time: 0.8s
train loss: 2.9246, val_loss: 3.4673
val loss decrease from 3.5139 to 3.4673, saving model to ./output/hyperparameters_experiment__gpu_bs10/model.pkl
2022-01-10 06:52:05 | epoch: 0011/50, training time: 18.3s, inference time: 0.8s
train loss: 2.9175, val_loss: 3.3819
val loss decrease from 3.4673 to 3.3819, saving model to ./output/hyperparameters_experiment__gpu_bs10/model.pkl
2022-01-10 06:52:24 | epoch: 0012/50, training time: 18.3s, inference time: 0.8s
train loss: 2.8323, val_loss: 3.6451
2022-01-10 06:52:43 | epoch: 0013/50, training time: 18.4s, inference time: 0.8s
train loss: 2.7876, val_loss: 3.4482
2022-01-10 06:53:02 | epoch: 0014/50, training time: 18.4s, inference time: 0.8s
train loss: 2.7740, val_loss: 3.4795
2022-01-10 06:53:21 | epoch: 0015/50, training time: 18.3s, inference time: 0.8s
train loss: 2.7465, val_loss: 3.4283
2022-01-10 06:53:40 | epoch: 0016/50, training time: 18.2s, inference time: 0.8s
train loss: 2.6678, val_loss: 3.5988
2022-01-10 06:53:59 | epoch: 0017/50, training time: 18.2s, inference time: 0.8s
train loss: 2.6202, val_loss: 3.4853
2022-01-10 06:54:18 | epoch: 0018/50, training time: 18.1s, inference time: 0.8s
train loss: 2.5308, val_loss: 3.6077
2022-01-10 06:54:37 | epoch: 0019/50, training time: 18.2s, inference time: 0.8s
train loss: 2.5392, val_loss: 3.6990
2022-01-10 06:54:56 | epoch: 0020/50, training time: 18.2s, inference time: 0.8s
train loss: 2.5046, val_loss: 3.5409
2022-01-10 06:55:15 | epoch: 0021/50, training time: 18.4s, inference time: 0.8s
train loss: 2.4096, val_loss: 3.6560
2022-01-10 06:55:35 | epoch: 0022/50, training time: 18.4s, inference time: 0.8s
train loss: 2.3629, val_loss: 3.5734
2022-01-10 06:55:54 | epoch: 0023/50, training time: 18.4s, inference time: 0.8s
train loss: 2.3509, val_loss: 3.7358
2022-01-10 06:56:13 | epoch: 0024/50, training time: 18.5s, inference time: 0.8s
train loss: 2.2650, val_loss: 3.6172
2022-01-10 06:56:32 | epoch: 0025/50, training time: 18.4s, inference time: 0.8s
train loss: 2.2051, val_loss: 3.7045
2022-01-10 06:56:51 | epoch: 0026/50, training time: 18.4s, inference time: 0.8s
train loss: 2.1583, val_loss: 3.8102
2022-01-10 06:57:10 | epoch: 0027/50, training time: 18.4s, inference time: 0.8s
train loss: 2.1416, val_loss: 3.8608
2022-01-10 06:57:30 | epoch: 0028/50, training time: 18.5s, inference time: 0.8s
train loss: 2.1308, val_loss: 3.6794
2022-01-10 06:57:49 | epoch: 0029/50, training time: 18.4s, inference time: 0.8s
train loss: 2.1166, val_loss: 4.0108
2022-01-10 06:58:08 | epoch: 0030/50, training time: 18.4s, inference time: 0.8s
train loss: 2.0410, val_loss: 3.9857
2022-01-10 06:58:27 | epoch: 0031/50, training time: 18.5s, inference time: 0.8s
train loss: 2.0073, val_loss: 3.8024
2022-01-10 06:58:47 | epoch: 0032/50, training time: 18.4s, inference time: 0.8s
train loss: 1.9653, val_loss: 3.9670
2022-01-10 06:59:06 | epoch: 0033/50, training time: 18.3s, inference time: 0.8s
train loss: 1.9205, val_loss: 4.2269
2022-01-10 06:59:25 | epoch: 0034/50, training time: 18.3s, inference time: 0.8s
train loss: 1.9435, val_loss: 4.2739
2022-01-10 06:59:44 | epoch: 0035/50, training time: 18.4s, inference time: 0.8s
train loss: 1.9019, val_loss: 4.2496
2022-01-10 07:00:03 | epoch: 0036/50, training time: 18.4s, inference time: 0.8s
train loss: 1.8417, val_loss: 4.2621
2022-01-10 07:00:22 | epoch: 0037/50, training time: 18.4s, inference time: 0.8s
train loss: 1.8582, val_loss: 4.2816
2022-01-10 07:00:41 | epoch: 0038/50, training time: 18.4s, inference time: 0.8s
train loss: 1.7854, val_loss: 4.3210
2022-01-10 07:01:01 | epoch: 0039/50, training time: 18.4s, inference time: 0.8s
train loss: 1.8036, val_loss: 4.2582
2022-01-10 07:01:20 | epoch: 0040/50, training time: 18.5s, inference time: 0.8s
train loss: 1.7989, val_loss: 4.1601
2022-01-10 07:01:39 | epoch: 0041/50, training time: 18.4s, inference time: 0.8s
train loss: 1.7837, val_loss: 4.2963
2022-01-10 07:01:58 | epoch: 0042/50, training time: 18.4s, inference time: 0.8s
train loss: 1.7621, val_loss: 4.1733
2022-01-10 07:02:17 | epoch: 0043/50, training time: 18.3s, inference time: 0.8s
train loss: 1.6943, val_loss: 4.1841
2022-01-10 07:02:36 | epoch: 0044/50, training time: 18.4s, inference time: 0.8s
train loss: 1.6754, val_loss: 4.1834
2022-01-10 07:02:56 | epoch: 0045/50, training time: 18.4s, inference time: 0.8s
train loss: 1.6722, val_loss: 4.2175
2022-01-10 07:03:15 | epoch: 0046/50, training time: 18.4s, inference time: 0.8s
train loss: 1.6520, val_loss: 4.5576
2022-01-10 07:03:34 | epoch: 0047/50, training time: 18.5s, inference time: 0.8s
train loss: 1.6671, val_loss: 4.2373
2022-01-10 07:03:53 | epoch: 0048/50, training time: 18.5s, inference time: 0.8s
train loss: 1.6366, val_loss: 4.3863
2022-01-10 07:04:13 | epoch: 0049/50, training time: 18.5s, inference time: 0.8s
train loss: 1.6196, val_loss: 4.4110
2022-01-10 07:04:32 | epoch: 0050/50, training time: 18.6s, inference time: 0.8s
train loss: 1.5927, val_loss: 4.3712
Training and validation are completed, and model has been stored as ./output/hyperparameters_experiment__gpu_bs10/model.pkl
**** testing model ****
loading model from ./output/hyperparameters_experiment__gpu_bs10/model.pkl
model restored!
evaluating...
testing time: 1.5s
                MAE		RMSE		MAPE
train            0.70		1.10		10.83%
val              1.28		2.11		20.28%
test             1.14		1.94		17.78%
performance in each prediction step
step: 01         1.14		1.94		17.78%
average:         1.14		1.94		17.78%
total time: 16.1min
