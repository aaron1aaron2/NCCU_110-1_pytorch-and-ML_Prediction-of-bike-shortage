K=8, L=1, SE_file='data/train_data/SE/basic/SE.txt', batch_size=24, d=8, decay_epoch=10, device='cuda', learning_rate=0.01, log_file='./output/length_of_history_steps_experiment__numhistory10/log.txt', max_epoch=50, model_file='./output/length_of_history_steps_experiment__numhistory10/model.pkl', num_his=10, num_pred=1, output_folder='./output/length_of_history_steps_experiment__numhistory10', patience=100, test_ratio=0.2, time_slot=10, traffic_file='data/train_data/data.h5', train_ratio=0.7, val_ratio=0.1, view_batch_freq=100
main output folder./output/length_of_history_steps_experiment__numhistory10
loading data...
trainX: torch.Size([3617, 10, 26])		 trainY: torch.Size([3617, 1, 26])
valX:   torch.Size([508, 10, 26])		valY:   torch.Size([508, 1, 26])
testX:   torch.Size([1026, 10, 26])		testY:   torch.Size([1026, 1, 26])
mean:   11.0460		std:   6.8492
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-01-11 10:33:34 | epoch: 0001/50, training time: 10.5s, inference time: 0.4s
train loss: 6.2693, val_loss: 5.7789
val loss decrease from inf to 5.7789, saving model to ./output/length_of_history_steps_experiment__numhistory10/model.pkl
2022-01-11 10:33:45 | epoch: 0002/50, training time: 10.3s, inference time: 0.4s
train loss: 4.2842, val_loss: 6.8018
2022-01-11 10:33:55 | epoch: 0003/50, training time: 10.3s, inference time: 0.4s
train loss: 3.5863, val_loss: 4.4567
val loss decrease from 5.7789 to 4.4567, saving model to ./output/length_of_history_steps_experiment__numhistory10/model.pkl
2022-01-11 10:34:06 | epoch: 0004/50, training time: 10.4s, inference time: 0.4s
train loss: 3.2800, val_loss: 4.4776
2022-01-11 10:34:17 | epoch: 0005/50, training time: 10.3s, inference time: 0.4s
train loss: 3.1873, val_loss: 3.9915
val loss decrease from 4.4567 to 3.9915, saving model to ./output/length_of_history_steps_experiment__numhistory10/model.pkl
2022-01-11 10:34:27 | epoch: 0006/50, training time: 10.3s, inference time: 0.4s
train loss: 3.0655, val_loss: 4.1401
2022-01-11 10:34:38 | epoch: 0007/50, training time: 10.4s, inference time: 0.4s
train loss: 3.0438, val_loss: 4.3144
2022-01-11 10:34:49 | epoch: 0008/50, training time: 10.4s, inference time: 0.4s
train loss: 2.9356, val_loss: 3.8861
val loss decrease from 3.9915 to 3.8861, saving model to ./output/length_of_history_steps_experiment__numhistory10/model.pkl
2022-01-11 10:35:00 | epoch: 0009/50, training time: 10.3s, inference time: 0.4s
train loss: 2.9407, val_loss: 4.0947
2022-01-11 10:35:10 | epoch: 0010/50, training time: 10.3s, inference time: 0.4s
train loss: 2.8431, val_loss: 4.8648
2022-01-11 10:35:21 | epoch: 0011/50, training time: 10.3s, inference time: 0.4s
train loss: 2.7608, val_loss: 4.1096
2022-01-11 10:35:32 | epoch: 0012/50, training time: 10.3s, inference time: 0.4s
train loss: 2.6878, val_loss: 4.0156
2022-01-11 10:35:43 | epoch: 0013/50, training time: 10.4s, inference time: 0.4s
train loss: 2.6755, val_loss: 3.8803
val loss decrease from 3.8861 to 3.8803, saving model to ./output/length_of_history_steps_experiment__numhistory10/model.pkl
2022-01-11 10:35:53 | epoch: 0014/50, training time: 10.3s, inference time: 0.4s
train loss: 2.6193, val_loss: 4.0321
2022-01-11 10:36:04 | epoch: 0015/50, training time: 10.3s, inference time: 0.4s
train loss: 2.5888, val_loss: 3.9694
2022-01-11 10:36:15 | epoch: 0016/50, training time: 10.3s, inference time: 0.4s
train loss: 2.4982, val_loss: 4.0448
2022-01-11 10:36:26 | epoch: 0017/50, training time: 10.4s, inference time: 0.4s
train loss: 2.4732, val_loss: 4.0806
2022-01-11 10:36:36 | epoch: 0018/50, training time: 10.3s, inference time: 0.4s
train loss: 2.4205, val_loss: 4.1067
2022-01-11 10:36:47 | epoch: 0019/50, training time: 10.3s, inference time: 0.4s
train loss: 2.4151, val_loss: 4.1576
2022-01-11 10:36:58 | epoch: 0020/50, training time: 10.3s, inference time: 0.4s
train loss: 2.3298, val_loss: 4.2385
2022-01-11 10:37:09 | epoch: 0021/50, training time: 10.3s, inference time: 0.4s
train loss: 2.2659, val_loss: 4.0149
2022-01-11 10:37:19 | epoch: 0022/50, training time: 10.3s, inference time: 0.4s
train loss: 2.2407, val_loss: 3.9407
2022-01-11 10:37:30 | epoch: 0023/50, training time: 10.4s, inference time: 0.4s
train loss: 2.1765, val_loss: 4.1194
2022-01-11 10:37:41 | epoch: 0024/50, training time: 10.4s, inference time: 0.4s
train loss: 2.1853, val_loss: 4.1188
2022-01-11 10:37:52 | epoch: 0025/50, training time: 10.4s, inference time: 0.4s
train loss: 2.1490, val_loss: 4.1500
2022-01-11 10:38:02 | epoch: 0026/50, training time: 10.3s, inference time: 0.4s
train loss: 2.1081, val_loss: 4.1281
2022-01-11 10:38:13 | epoch: 0027/50, training time: 10.2s, inference time: 0.4s
train loss: 2.0843, val_loss: 4.0966
2022-01-11 10:38:24 | epoch: 0028/50, training time: 10.3s, inference time: 0.4s
train loss: 1.9844, val_loss: 4.0809
2022-01-11 10:38:34 | epoch: 0029/50, training time: 10.3s, inference time: 0.4s
train loss: 2.0314, val_loss: 4.3926
2022-01-11 10:38:45 | epoch: 0030/50, training time: 10.3s, inference time: 0.4s
train loss: 1.9803, val_loss: 4.2404
2022-01-11 10:38:56 | epoch: 0031/50, training time: 10.3s, inference time: 0.4s
train loss: 1.9108, val_loss: 4.3428
2022-01-11 10:39:06 | epoch: 0032/50, training time: 10.3s, inference time: 0.4s
train loss: 1.8546, val_loss: 4.2788
2022-01-11 10:39:17 | epoch: 0033/50, training time: 10.3s, inference time: 0.4s
train loss: 1.8068, val_loss: 4.3655
2022-01-11 10:39:28 | epoch: 0034/50, training time: 10.3s, inference time: 0.4s
train loss: 1.7853, val_loss: 4.1559
2022-01-11 10:39:38 | epoch: 0035/50, training time: 10.2s, inference time: 0.4s
train loss: 1.7852, val_loss: 4.5664
2022-01-11 10:39:49 | epoch: 0036/50, training time: 10.3s, inference time: 0.4s
train loss: 1.7816, val_loss: 4.4066
2022-01-11 10:40:00 | epoch: 0037/50, training time: 10.3s, inference time: 0.4s
train loss: 1.7497, val_loss: 4.1048
2022-01-11 10:40:11 | epoch: 0038/50, training time: 10.3s, inference time: 0.4s
train loss: 1.7270, val_loss: 4.2936
2022-01-11 10:40:21 | epoch: 0039/50, training time: 10.3s, inference time: 0.4s
train loss: 1.7435, val_loss: 4.4445
2022-01-11 10:40:32 | epoch: 0040/50, training time: 10.3s, inference time: 0.4s
train loss: 1.7975, val_loss: 4.1792
2022-01-11 10:40:43 | epoch: 0041/50, training time: 10.4s, inference time: 0.4s
train loss: 1.6185, val_loss: 4.4277
2022-01-11 10:40:54 | epoch: 0042/50, training time: 10.3s, inference time: 0.4s
train loss: 1.6041, val_loss: 4.3837
2022-01-11 10:41:04 | epoch: 0043/50, training time: 10.3s, inference time: 0.4s
train loss: 1.5654, val_loss: 4.6329
2022-01-11 10:41:15 | epoch: 0044/50, training time: 10.3s, inference time: 0.4s
train loss: 1.5835, val_loss: 4.3085
2022-01-11 10:41:26 | epoch: 0045/50, training time: 10.3s, inference time: 0.4s
train loss: 1.5460, val_loss: 4.2759
2022-01-11 10:41:36 | epoch: 0046/50, training time: 10.4s, inference time: 0.4s
train loss: 1.5062, val_loss: 4.4312
2022-01-11 10:41:47 | epoch: 0047/50, training time: 10.3s, inference time: 0.4s
train loss: 1.4921, val_loss: 4.5811
2022-01-11 10:41:58 | epoch: 0048/50, training time: 10.3s, inference time: 0.4s
train loss: 1.4818, val_loss: 4.5089
2022-01-11 10:42:09 | epoch: 0049/50, training time: 10.3s, inference time: 0.4s
train loss: 1.4853, val_loss: 4.5510
2022-01-11 10:42:19 | epoch: 0050/50, training time: 10.3s, inference time: 0.4s
train loss: 1.4689, val_loss: 4.4847
Training and validation are completed, and model has been stored as ./output/length_of_history_steps_experiment__numhistory10/model.pkl
**** testing model ****
loading model from ./output/length_of_history_steps_experiment__numhistory10/model.pkl
model restored!
evaluating...
testing time: 0.8s
                MAE		RMSE		MAPE
train            0.69		1.10		10.43%
val              1.30		2.14		18.73%
test             1.10		1.98		16.66%
performance in each prediction step
step: 01         1.10		1.98		16.66%
average:         1.10		1.98		16.66%
total time: 9.0min
