K=8, L=1, SE_file='data/train_data/SE/basic/SE.txt', batch_size=6, d=8, decay_epoch=10, device='cuda', learning_rate=0.001, log_file='./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/log.txt', max_epoch=50, model_file='./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl', num_his=5, num_pred=1, output_folder='./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50', patience=100, test_ratio=0.2, time_slot=10, traffic_file='data/train_data/data.h5', train_ratio=0.7, val_ratio=0.1, view_batch_freq=100
main output folder./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50
loading data...
trainX: torch.Size([3622, 5, 26])		 trainY: torch.Size([3622, 1, 26])
valX:   torch.Size([513, 5, 26])		valY:   torch.Size([513, 1, 26])
testX:   torch.Size([1031, 5, 26])		testY:   torch.Size([1031, 1, 26])
mean:   11.0472		std:   6.8502
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-01-13 02:28:08 | epoch: 0001/50, training time: 24.2s, inference time: 1.1s
train loss: 4.7635, val_loss: 4.0444
val loss decrease from inf to 4.0444, saving model to ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
2022-01-13 02:28:33 | epoch: 0002/50, training time: 23.7s, inference time: 1.1s
train loss: 3.6435, val_loss: 3.7743
val loss decrease from 4.0444 to 3.7743, saving model to ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
2022-01-13 02:28:57 | epoch: 0003/50, training time: 23.4s, inference time: 1.1s
train loss: 3.5221, val_loss: 3.6735
val loss decrease from 3.7743 to 3.6735, saving model to ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
2022-01-13 02:29:22 | epoch: 0004/50, training time: 23.4s, inference time: 1.1s
train loss: 3.4461, val_loss: 3.5664
val loss decrease from 3.6735 to 3.5664, saving model to ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
2022-01-13 02:29:46 | epoch: 0005/50, training time: 23.0s, inference time: 1.1s
train loss: 3.3459, val_loss: 3.5451
val loss decrease from 3.5664 to 3.5451, saving model to ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
2022-01-13 02:30:11 | epoch: 0006/50, training time: 24.1s, inference time: 1.1s
train loss: 3.2853, val_loss: 3.5691
2022-01-13 02:30:35 | epoch: 0007/50, training time: 23.0s, inference time: 1.1s
train loss: 3.2485, val_loss: 3.5805
2022-01-13 02:30:59 | epoch: 0008/50, training time: 22.9s, inference time: 1.1s
train loss: 3.1913, val_loss: 3.4567
val loss decrease from 3.5451 to 3.4567, saving model to ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
2022-01-13 02:31:24 | epoch: 0009/50, training time: 23.3s, inference time: 1.1s
train loss: 3.1848, val_loss: 3.5724
2022-01-13 02:31:49 | epoch: 0010/50, training time: 23.8s, inference time: 1.1s
train loss: 3.1619, val_loss: 3.5306
2022-01-13 02:32:13 | epoch: 0011/50, training time: 23.2s, inference time: 1.1s
train loss: 3.0198, val_loss: 3.5692
2022-01-13 02:32:37 | epoch: 0012/50, training time: 23.2s, inference time: 1.1s
train loss: 3.0307, val_loss: 3.4250
val loss decrease from 3.4567 to 3.4250, saving model to ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
2022-01-13 02:33:01 | epoch: 0013/50, training time: 22.5s, inference time: 1.0s
train loss: 2.9951, val_loss: 3.5382
2022-01-13 02:33:25 | epoch: 0014/50, training time: 23.0s, inference time: 1.1s
train loss: 2.9353, val_loss: 3.5328
2022-01-13 02:33:49 | epoch: 0015/50, training time: 22.9s, inference time: 1.1s
train loss: 2.9031, val_loss: 3.5809
2022-01-13 02:34:13 | epoch: 0016/50, training time: 22.9s, inference time: 1.1s
train loss: 2.8727, val_loss: 3.5303
2022-01-13 02:34:37 | epoch: 0017/50, training time: 22.9s, inference time: 1.1s
train loss: 2.8251, val_loss: 3.4540
2022-01-13 02:35:01 | epoch: 0018/50, training time: 22.9s, inference time: 1.1s
train loss: 2.7727, val_loss: 3.4550
2022-01-13 02:35:25 | epoch: 0019/50, training time: 22.9s, inference time: 1.1s
train loss: 2.7882, val_loss: 3.6861
2022-01-13 02:35:49 | epoch: 0020/50, training time: 23.4s, inference time: 1.1s
train loss: 2.6841, val_loss: 3.4431
2022-01-13 02:36:14 | epoch: 0021/50, training time: 23.1s, inference time: 1.2s
train loss: 2.6401, val_loss: 3.4749
2022-01-13 02:36:39 | epoch: 0022/50, training time: 24.0s, inference time: 1.0s
train loss: 2.5701, val_loss: 3.6095
2022-01-13 02:37:03 | epoch: 0023/50, training time: 23.1s, inference time: 1.1s
train loss: 2.5515, val_loss: 3.4875
2022-01-13 02:37:27 | epoch: 0024/50, training time: 23.2s, inference time: 1.2s
train loss: 2.5029, val_loss: 3.6449
2022-01-13 02:37:51 | epoch: 0025/50, training time: 23.0s, inference time: 1.1s
train loss: 2.4819, val_loss: 3.6530
2022-01-13 02:38:16 | epoch: 0026/50, training time: 23.3s, inference time: 1.1s
train loss: 2.4439, val_loss: 3.7177
2022-01-13 02:38:40 | epoch: 0027/50, training time: 23.5s, inference time: 1.1s
train loss: 2.3891, val_loss: 3.7393
2022-01-13 02:39:04 | epoch: 0028/50, training time: 22.8s, inference time: 1.1s
train loss: 2.3815, val_loss: 3.8404
2022-01-13 02:39:29 | epoch: 0029/50, training time: 23.3s, inference time: 1.1s
train loss: 2.3896, val_loss: 4.1504
2022-01-13 02:39:54 | epoch: 0030/50, training time: 23.9s, inference time: 1.1s
train loss: 2.2757, val_loss: 3.9088
2022-01-13 02:40:18 | epoch: 0031/50, training time: 23.8s, inference time: 1.1s
train loss: 2.2397, val_loss: 4.0794
2022-01-13 02:40:43 | epoch: 0032/50, training time: 23.8s, inference time: 1.1s
train loss: 2.2074, val_loss: 3.9066
2022-01-13 02:41:08 | epoch: 0033/50, training time: 23.7s, inference time: 1.1s
train loss: 2.1786, val_loss: 3.8686
2022-01-13 02:41:33 | epoch: 0034/50, training time: 23.4s, inference time: 1.2s
train loss: 2.1561, val_loss: 3.8836
2022-01-13 02:41:57 | epoch: 0035/50, training time: 23.6s, inference time: 1.1s
train loss: 2.1472, val_loss: 3.9532
2022-01-13 02:42:22 | epoch: 0036/50, training time: 23.7s, inference time: 1.1s
train loss: 2.1444, val_loss: 4.0709
2022-01-13 02:42:47 | epoch: 0037/50, training time: 23.4s, inference time: 1.2s
train loss: 2.1307, val_loss: 4.1897
2022-01-13 02:43:11 | epoch: 0038/50, training time: 23.3s, inference time: 1.1s
train loss: 2.0939, val_loss: 4.1099
2022-01-13 02:43:36 | epoch: 0039/50, training time: 23.6s, inference time: 1.2s
train loss: 2.1000, val_loss: 3.9186
2022-01-13 02:44:00 | epoch: 0040/50, training time: 23.3s, inference time: 1.1s
train loss: 2.0684, val_loss: 4.1008
2022-01-13 02:44:25 | epoch: 0041/50, training time: 23.2s, inference time: 1.1s
train loss: 2.0092, val_loss: 3.9676
2022-01-13 02:44:49 | epoch: 0042/50, training time: 23.0s, inference time: 1.1s
train loss: 1.9832, val_loss: 4.2605
2022-01-13 02:45:13 | epoch: 0043/50, training time: 22.8s, inference time: 1.1s
train loss: 2.0140, val_loss: 4.0669
2022-01-13 02:45:38 | epoch: 0044/50, training time: 23.5s, inference time: 1.2s
train loss: 1.9590, val_loss: 4.2034
2022-01-13 02:46:02 | epoch: 0045/50, training time: 22.9s, inference time: 1.1s
train loss: 1.9711, val_loss: 4.2003
2022-01-13 02:46:26 | epoch: 0046/50, training time: 23.1s, inference time: 1.1s
train loss: 1.9441, val_loss: 4.1884
2022-01-13 02:46:50 | epoch: 0047/50, training time: 23.1s, inference time: 1.1s
train loss: 1.9273, val_loss: 4.1946
2022-01-13 02:47:13 | epoch: 0048/50, training time: 22.5s, inference time: 1.1s
train loss: 1.8898, val_loss: 4.4709
2022-01-13 02:47:37 | epoch: 0049/50, training time: 22.6s, inference time: 1.1s
train loss: 1.8839, val_loss: 4.1632
2022-01-13 02:48:01 | epoch: 0050/50, training time: 22.7s, inference time: 1.1s
train loss: 1.8638, val_loss: 4.2563
Training and validation are completed, and model has been stored as ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
**** testing model ****
loading model from ./output/cpu_gpu_experiment__gpu(Tesla-T4)_epoch50/model.pkl
model restored!
evaluating...
testing time: 2.1s
                MAE		RMSE		MAPE
train            0.72		1.15		11.36%
val              1.25		2.09		18.43%
test             1.10		1.95		16.49%
performance in each prediction step
step: 01         1.10		1.95		16.49%
average:         1.10		1.95		16.49%
total time: 20.5min
