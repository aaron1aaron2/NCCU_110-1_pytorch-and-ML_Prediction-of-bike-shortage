K=8, L=1, SE_file='data/train_data/SE/basic/SE.txt', batch_size=24, d=8, decay_epoch=10, device='cuda', learning_rate=0.01, log_file='./output/hyperparameters_experiment__gpu_lr0.01/log.txt', max_epoch=100, model_file='./output/hyperparameters_experiment__gpu_lr0.01/model.pkl', num_his=5, num_pred=1, output_folder='./output/hyperparameters_experiment__gpu_lr0.01', patience=100, test_ratio=0.2, time_slot=10, traffic_file='data/train_data/data.h5', train_ratio=0.7, val_ratio=0.1, view_batch_freq=100
main output folder./output/hyperparameters_experiment__gpu_lr0.01
loading data...
trainX: torch.Size([3622, 5, 26])		 trainY: torch.Size([3622, 1, 26])
valX:   torch.Size([513, 5, 26])		valY:   torch.Size([513, 1, 26])
testX:   torch.Size([1031, 5, 26])		testY:   torch.Size([1031, 1, 26])
mean:   11.0472		std:   6.8502
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-01-10 23:18:10 | epoch: 0001/100, training time: 17.1s, inference time: 0.6s
train loss: 4.2633, val_loss: 3.6844
val loss decrease from inf to 3.6844, saving model to ./output/hyperparameters_experiment__gpu_lr0.01/model.pkl
2022-01-10 23:18:23 | epoch: 0002/100, training time: 12.7s, inference time: 0.7s
train loss: 3.1461, val_loss: 4.1423
2022-01-10 23:18:38 | epoch: 0003/100, training time: 14.2s, inference time: 0.7s
train loss: 3.0496, val_loss: 3.7254
2022-01-10 23:18:52 | epoch: 0004/100, training time: 13.5s, inference time: 0.8s
train loss: 2.9644, val_loss: 3.7072
2022-01-10 23:19:07 | epoch: 0005/100, training time: 13.6s, inference time: 0.7s
train loss: 2.9188, val_loss: 3.4631
val loss decrease from 3.6844 to 3.4631, saving model to ./output/hyperparameters_experiment__gpu_lr0.01/model.pkl
2022-01-10 23:19:21 | epoch: 0006/100, training time: 14.2s, inference time: 0.7s
train loss: 2.8545, val_loss: 3.4462
val loss decrease from 3.4631 to 3.4462, saving model to ./output/hyperparameters_experiment__gpu_lr0.01/model.pkl
2022-01-10 23:19:36 | epoch: 0007/100, training time: 14.0s, inference time: 0.7s
train loss: 2.8350, val_loss: 3.5703
2022-01-10 23:19:53 | epoch: 0008/100, training time: 15.7s, inference time: 0.7s
train loss: 2.7655, val_loss: 3.5119
2022-01-10 23:20:06 | epoch: 0009/100, training time: 13.0s, inference time: 0.7s
train loss: 2.7528, val_loss: 3.4796
2022-01-10 23:20:20 | epoch: 0010/100, training time: 13.4s, inference time: 0.7s
train loss: 2.7171, val_loss: 3.5316
2022-01-10 23:20:34 | epoch: 0011/100, training time: 13.3s, inference time: 0.8s
train loss: 2.6857, val_loss: 3.4671
2022-01-10 23:20:49 | epoch: 0012/100, training time: 13.5s, inference time: 0.7s
train loss: 2.6637, val_loss: 3.6044
2022-01-10 23:21:04 | epoch: 0013/100, training time: 14.1s, inference time: 0.7s
train loss: 2.6292, val_loss: 3.4814
2022-01-10 23:21:20 | epoch: 0014/100, training time: 15.3s, inference time: 0.8s
train loss: 2.5603, val_loss: 3.4426
val loss decrease from 3.4462 to 3.4426, saving model to ./output/hyperparameters_experiment__gpu_lr0.01/model.pkl
2022-01-10 23:21:37 | epoch: 0015/100, training time: 16.4s, inference time: 0.9s
train loss: 2.5193, val_loss: 3.7716
2022-01-10 23:21:52 | epoch: 0016/100, training time: 14.6s, inference time: 0.7s
train loss: 2.5359, val_loss: 3.7417
2022-01-10 23:22:07 | epoch: 0017/100, training time: 13.6s, inference time: 0.7s
train loss: 2.4960, val_loss: 3.6242
2022-01-10 23:22:21 | epoch: 0018/100, training time: 14.0s, inference time: 0.7s
train loss: 2.4410, val_loss: 3.8158
2022-01-10 23:22:35 | epoch: 0019/100, training time: 13.1s, inference time: 0.7s
train loss: 2.4181, val_loss: 3.7722
2022-01-10 23:22:49 | epoch: 0020/100, training time: 13.4s, inference time: 0.7s
train loss: 2.4193, val_loss: 3.8433
2022-01-10 23:23:03 | epoch: 0021/100, training time: 13.2s, inference time: 0.7s
train loss: 2.3179, val_loss: 4.0181
2022-01-10 23:23:18 | epoch: 0022/100, training time: 14.4s, inference time: 0.7s
train loss: 2.2768, val_loss: 3.6331
2022-01-10 23:23:33 | epoch: 0023/100, training time: 14.0s, inference time: 0.7s
train loss: 2.2778, val_loss: 3.8457
2022-01-10 23:23:48 | epoch: 0024/100, training time: 14.9s, inference time: 0.7s
train loss: 2.2804, val_loss: 3.6558
2022-01-10 23:24:02 | epoch: 0025/100, training time: 13.4s, inference time: 0.7s
train loss: 2.2007, val_loss: 4.0529
2022-01-10 23:24:17 | epoch: 0026/100, training time: 13.4s, inference time: 0.8s
train loss: 2.1785, val_loss: 3.8470
2022-01-10 23:24:31 | epoch: 0027/100, training time: 13.3s, inference time: 0.7s
train loss: 2.1397, val_loss: 4.0756
2022-01-10 23:24:46 | epoch: 0028/100, training time: 15.0s, inference time: 0.7s
train loss: 2.1085, val_loss: 4.2528
2022-01-10 23:25:00 | epoch: 0029/100, training time: 13.6s, inference time: 0.7s
train loss: 2.0708, val_loss: 3.9473
2022-01-10 23:25:15 | epoch: 0030/100, training time: 13.5s, inference time: 0.7s
train loss: 2.0512, val_loss: 3.9103
2022-01-10 23:25:28 | epoch: 0031/100, training time: 12.9s, inference time: 0.7s
train loss: 1.9808, val_loss: 4.0428
2022-01-10 23:25:42 | epoch: 0032/100, training time: 13.2s, inference time: 0.7s
train loss: 1.9318, val_loss: 3.9547
2022-01-10 23:25:56 | epoch: 0033/100, training time: 13.1s, inference time: 0.7s
train loss: 1.9074, val_loss: 4.0703
2022-01-10 23:26:10 | epoch: 0034/100, training time: 13.7s, inference time: 0.7s
train loss: 1.8922, val_loss: 4.2755
2022-01-10 23:26:26 | epoch: 0035/100, training time: 15.2s, inference time: 0.7s
train loss: 1.8810, val_loss: 4.1182
2022-01-10 23:26:41 | epoch: 0036/100, training time: 13.5s, inference time: 0.7s
train loss: 1.8438, val_loss: 4.5286
2022-01-10 23:26:55 | epoch: 0037/100, training time: 13.3s, inference time: 0.7s
train loss: 1.8036, val_loss: 4.2973
2022-01-10 23:27:08 | epoch: 0038/100, training time: 13.1s, inference time: 0.7s
train loss: 1.7740, val_loss: 4.0701
2022-01-10 23:27:23 | epoch: 0039/100, training time: 13.7s, inference time: 0.7s
train loss: 1.7902, val_loss: 4.1590
2022-01-10 23:27:37 | epoch: 0040/100, training time: 13.0s, inference time: 0.7s
train loss: 1.7695, val_loss: 4.8410
2022-01-10 23:27:50 | epoch: 0041/100, training time: 13.1s, inference time: 0.7s
train loss: 1.6963, val_loss: 4.2860
2022-01-10 23:28:04 | epoch: 0042/100, training time: 13.0s, inference time: 0.7s
train loss: 1.6875, val_loss: 4.3010
2022-01-10 23:28:20 | epoch: 0043/100, training time: 15.2s, inference time: 0.7s
train loss: 1.6334, val_loss: 4.2192
2022-01-10 23:28:34 | epoch: 0044/100, training time: 13.2s, inference time: 0.7s
train loss: 1.6404, val_loss: 4.1790
2022-01-10 23:28:47 | epoch: 0045/100, training time: 13.0s, inference time: 0.7s
train loss: 1.5816, val_loss: 4.2729
2022-01-10 23:29:01 | epoch: 0046/100, training time: 12.9s, inference time: 0.7s
train loss: 1.5713, val_loss: 4.3113
2022-01-10 23:29:15 | epoch: 0047/100, training time: 13.0s, inference time: 0.7s
train loss: 1.5463, val_loss: 4.2761
2022-01-10 23:29:28 | epoch: 0048/100, training time: 13.0s, inference time: 0.7s
train loss: 1.5546, val_loss: 4.4772
2022-01-10 23:29:42 | epoch: 0049/100, training time: 13.1s, inference time: 0.7s
train loss: 1.5475, val_loss: 4.5094
2022-01-10 23:29:56 | epoch: 0050/100, training time: 13.0s, inference time: 0.7s
train loss: 1.5243, val_loss: 4.3153
2022-01-10 23:30:10 | epoch: 0051/100, training time: 13.2s, inference time: 0.7s
train loss: 1.4781, val_loss: 4.8104
2022-01-10 23:30:23 | epoch: 0052/100, training time: 13.1s, inference time: 0.7s
train loss: 1.4407, val_loss: 4.4493
2022-01-10 23:30:37 | epoch: 0053/100, training time: 13.2s, inference time: 0.7s
train loss: 1.4587, val_loss: 4.3023
2022-01-10 23:30:51 | epoch: 0054/100, training time: 13.2s, inference time: 0.7s
train loss: 1.4337, val_loss: 4.8847
2022-01-10 23:31:06 | epoch: 0055/100, training time: 13.7s, inference time: 0.8s
train loss: 1.4119, val_loss: 4.1847
2022-01-10 23:31:22 | epoch: 0056/100, training time: 15.4s, inference time: 0.7s
train loss: 1.4337, val_loss: 4.6960
2022-01-10 23:31:38 | epoch: 0057/100, training time: 15.1s, inference time: 0.8s
train loss: 1.4153, val_loss: 4.4221
2022-01-10 23:31:52 | epoch: 0058/100, training time: 14.1s, inference time: 0.7s
train loss: 1.3888, val_loss: 4.5901
2022-01-10 23:32:08 | epoch: 0059/100, training time: 15.2s, inference time: 0.7s
train loss: 1.3972, val_loss: 4.4286
2022-01-10 23:32:24 | epoch: 0060/100, training time: 15.0s, inference time: 0.7s
train loss: 1.3402, val_loss: 4.6381
2022-01-10 23:32:38 | epoch: 0061/100, training time: 13.2s, inference time: 0.8s
train loss: 1.2790, val_loss: 4.9576
2022-01-10 23:32:54 | epoch: 0062/100, training time: 15.6s, inference time: 0.8s
train loss: 1.3192, val_loss: 4.5585
2022-01-10 23:33:09 | epoch: 0063/100, training time: 13.9s, inference time: 0.8s
train loss: 1.2681, val_loss: 4.8388
2022-01-10 23:33:24 | epoch: 0064/100, training time: 14.0s, inference time: 0.8s
train loss: 1.3003, val_loss: 4.6924
2022-01-10 23:33:39 | epoch: 0065/100, training time: 14.2s, inference time: 0.7s
train loss: 1.2338, val_loss: 4.8661
2022-01-10 23:33:53 | epoch: 0066/100, training time: 13.3s, inference time: 0.7s
train loss: 1.2320, val_loss: 4.7940
2022-01-10 23:34:08 | epoch: 0067/100, training time: 14.2s, inference time: 0.8s
train loss: 1.2371, val_loss: 4.4755
2022-01-10 23:34:22 | epoch: 0068/100, training time: 13.9s, inference time: 0.7s
train loss: 1.2349, val_loss: 4.9877
2022-01-10 23:34:36 | epoch: 0069/100, training time: 13.2s, inference time: 0.7s
train loss: 1.2102, val_loss: 4.8947
2022-01-10 23:34:51 | epoch: 0070/100, training time: 13.8s, inference time: 0.7s
train loss: 1.1570, val_loss: 4.7423
2022-01-10 23:35:04 | epoch: 0071/100, training time: 12.9s, inference time: 0.7s
train loss: 1.1783, val_loss: 4.9054
2022-01-10 23:35:18 | epoch: 0072/100, training time: 13.1s, inference time: 0.7s
train loss: 1.1505, val_loss: 4.4717
2022-01-10 23:35:32 | epoch: 0073/100, training time: 13.5s, inference time: 0.7s
train loss: 1.1274, val_loss: 4.8470
2022-01-10 23:35:46 | epoch: 0074/100, training time: 13.2s, inference time: 0.7s
train loss: 1.1133, val_loss: 4.8250
2022-01-10 23:36:00 | epoch: 0075/100, training time: 13.0s, inference time: 0.7s
train loss: 1.1229, val_loss: 4.6472
2022-01-10 23:36:14 | epoch: 0076/100, training time: 13.5s, inference time: 0.7s
train loss: 1.1066, val_loss: 4.9870
2022-01-10 23:36:29 | epoch: 0077/100, training time: 14.2s, inference time: 0.8s
train loss: 1.1239, val_loss: 5.0184
2022-01-10 23:36:43 | epoch: 0078/100, training time: 13.3s, inference time: 0.7s
train loss: 1.0828, val_loss: 4.9810
2022-01-10 23:36:57 | epoch: 0079/100, training time: 13.1s, inference time: 0.7s
train loss: 1.0793, val_loss: 4.8246
2022-01-10 23:37:12 | epoch: 0080/100, training time: 14.3s, inference time: 0.7s
train loss: 1.0496, val_loss: 4.7204
2022-01-10 23:37:29 | epoch: 0081/100, training time: 16.0s, inference time: 0.7s
train loss: 1.0307, val_loss: 4.7444
2022-01-10 23:37:43 | epoch: 0082/100, training time: 13.4s, inference time: 0.7s
train loss: 1.0131, val_loss: 4.6008
2022-01-10 23:37:57 | epoch: 0083/100, training time: 13.3s, inference time: 0.7s
train loss: 1.0171, val_loss: 4.6632
2022-01-10 23:38:11 | epoch: 0084/100, training time: 13.0s, inference time: 0.7s
train loss: 0.9752, val_loss: 4.6739
2022-01-10 23:38:24 | epoch: 0085/100, training time: 12.9s, inference time: 0.7s
train loss: 1.0176, val_loss: 4.6518
2022-01-10 23:38:38 | epoch: 0086/100, training time: 13.0s, inference time: 0.7s
train loss: 1.0335, val_loss: 4.8387
2022-01-10 23:38:53 | epoch: 0087/100, training time: 14.1s, inference time: 0.9s
train loss: 0.9922, val_loss: 4.6980
2022-01-10 23:39:09 | epoch: 0088/100, training time: 15.5s, inference time: 0.7s
train loss: 1.0071, val_loss: 4.9481
2022-01-10 23:39:24 | epoch: 0089/100, training time: 14.0s, inference time: 0.7s
train loss: 0.9696, val_loss: 4.7832
2022-01-10 23:39:38 | epoch: 0090/100, training time: 13.5s, inference time: 0.7s
train loss: 0.9532, val_loss: 4.8876
2022-01-10 23:39:52 | epoch: 0091/100, training time: 13.7s, inference time: 0.9s
train loss: 0.9511, val_loss: 4.9081
2022-01-10 23:40:07 | epoch: 0092/100, training time: 13.7s, inference time: 0.7s
train loss: 0.9025, val_loss: 4.7496
2022-01-10 23:40:21 | epoch: 0093/100, training time: 13.2s, inference time: 0.7s
train loss: 0.9330, val_loss: 4.7801
2022-01-10 23:40:35 | epoch: 0094/100, training time: 13.2s, inference time: 0.7s
train loss: 0.9236, val_loss: 4.7219
2022-01-10 23:40:49 | epoch: 0095/100, training time: 13.5s, inference time: 0.7s
train loss: 0.9171, val_loss: 4.7933
2022-01-10 23:41:03 | epoch: 0096/100, training time: 13.4s, inference time: 0.7s
train loss: 0.9114, val_loss: 5.1524
2022-01-10 23:41:17 | epoch: 0097/100, training time: 13.3s, inference time: 0.7s
train loss: 0.9164, val_loss: 4.8983
2022-01-10 23:41:31 | epoch: 0098/100, training time: 13.1s, inference time: 0.7s
train loss: 0.9018, val_loss: 4.9538
2022-01-10 23:41:44 | epoch: 0099/100, training time: 13.1s, inference time: 0.7s
train loss: 0.8810, val_loss: 4.9819
2022-01-10 23:41:58 | epoch: 0100/100, training time: 13.1s, inference time: 0.7s
train loss: 0.9137, val_loss: 4.7790
Training and validation are completed, and model has been stored as ./output/hyperparameters_experiment__gpu_lr0.01/model.pkl
**** testing model ****
loading model from ./output/hyperparameters_experiment__gpu_lr0.01/model.pkl
model restored!
evaluating...
testing time: 1.3s
                MAE		RMSE		MAPE
train            0.57		0.83		9.39%
val              1.38		2.21		21.71%
test             1.22		2.04		20.49%
performance in each prediction step
step: 01         1.22		2.04		20.49%
average:         1.22		2.04		20.49%
total time: 24.2min
