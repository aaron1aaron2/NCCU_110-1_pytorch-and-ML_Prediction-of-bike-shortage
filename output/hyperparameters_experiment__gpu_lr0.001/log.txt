K=8, L=1, SE_file='data/train_data/SE/basic/SE.txt', batch_size=24, d=8, decay_epoch=10, device='cuda', learning_rate=0.001, log_file='./output/hyperparameters_experiment__gpu_lr0.001/log.txt', max_epoch=100, model_file='./output/hyperparameters_experiment__gpu_lr0.001/model.pkl', num_his=5, num_pred=1, output_folder='./output/hyperparameters_experiment__gpu_lr0.001', patience=100, test_ratio=0.2, time_slot=10, traffic_file='data/train_data/data.h5', train_ratio=0.7, val_ratio=0.1, view_batch_freq=100
main output folder./output/hyperparameters_experiment__gpu_lr0.001
loading data...
trainX: torch.Size([3622, 5, 26])		 trainY: torch.Size([3622, 1, 26])
valX:   torch.Size([513, 5, 26])		valY:   torch.Size([513, 1, 26])
testX:   torch.Size([1031, 5, 26])		testY:   torch.Size([1031, 1, 26])
mean:   11.0472		std:   6.8502
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-01-11 10:18:55 | epoch: 0001/100, training time: 18.9s, inference time: 0.6s
train loss: 5.3605, val_loss: 4.0889
val loss decrease from inf to 4.0889, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:19:10 | epoch: 0002/100, training time: 13.8s, inference time: 1.0s
train loss: 3.4161, val_loss: 3.8793
val loss decrease from 4.0889 to 3.8793, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:19:25 | epoch: 0003/100, training time: 14.2s, inference time: 0.7s
train loss: 3.2981, val_loss: 3.8879
2022-01-11 10:19:39 | epoch: 0004/100, training time: 13.5s, inference time: 0.7s
train loss: 3.2201, val_loss: 3.7126
val loss decrease from 3.8793 to 3.7126, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:19:56 | epoch: 0005/100, training time: 16.4s, inference time: 0.8s
train loss: 3.1250, val_loss: 3.7938
2022-01-11 10:20:10 | epoch: 0006/100, training time: 13.5s, inference time: 0.7s
train loss: 3.0725, val_loss: 3.6415
val loss decrease from 3.7126 to 3.6415, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:20:24 | epoch: 0007/100, training time: 13.2s, inference time: 0.7s
train loss: 3.0025, val_loss: 3.6396
val loss decrease from 3.6415 to 3.6396, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:20:40 | epoch: 0008/100, training time: 15.0s, inference time: 0.7s
train loss: 2.9573, val_loss: 3.5624
val loss decrease from 3.6396 to 3.5624, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:20:54 | epoch: 0009/100, training time: 13.3s, inference time: 0.7s
train loss: 2.8961, val_loss: 3.5429
val loss decrease from 3.5624 to 3.5429, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:21:08 | epoch: 0010/100, training time: 13.7s, inference time: 0.6s
train loss: 2.8574, val_loss: 3.5428
val loss decrease from 3.5429 to 3.5428, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:21:23 | epoch: 0011/100, training time: 13.6s, inference time: 0.8s
train loss: 2.7887, val_loss: 3.5026
val loss decrease from 3.5428 to 3.5026, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:21:37 | epoch: 0012/100, training time: 13.7s, inference time: 0.8s
train loss: 2.7485, val_loss: 3.4549
val loss decrease from 3.5026 to 3.4549, saving model to ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
2022-01-11 10:21:53 | epoch: 0013/100, training time: 14.8s, inference time: 0.7s
train loss: 2.6983, val_loss: 3.5216
2022-01-11 10:22:08 | epoch: 0014/100, training time: 15.1s, inference time: 0.8s
train loss: 2.6444, val_loss: 3.5802
2022-01-11 10:22:27 | epoch: 0015/100, training time: 17.2s, inference time: 0.8s
train loss: 2.6431, val_loss: 3.7939
2022-01-11 10:22:41 | epoch: 0016/100, training time: 13.9s, inference time: 0.7s
train loss: 2.5534, val_loss: 3.5126
2022-01-11 10:22:55 | epoch: 0017/100, training time: 12.9s, inference time: 0.9s
train loss: 2.5146, val_loss: 3.5679
2022-01-11 10:23:11 | epoch: 0018/100, training time: 14.8s, inference time: 0.8s
train loss: 2.4761, val_loss: 3.6235
2022-01-11 10:23:26 | epoch: 0019/100, training time: 14.5s, inference time: 0.8s
train loss: 2.4496, val_loss: 3.6388
2022-01-11 10:23:40 | epoch: 0020/100, training time: 13.1s, inference time: 0.7s
train loss: 2.3869, val_loss: 3.5847
2022-01-11 10:23:54 | epoch: 0021/100, training time: 13.3s, inference time: 0.7s
train loss: 2.3106, val_loss: 3.6732
2022-01-11 10:24:07 | epoch: 0022/100, training time: 12.8s, inference time: 0.7s
train loss: 2.2555, val_loss: 3.7918
2022-01-11 10:24:21 | epoch: 0023/100, training time: 13.1s, inference time: 0.7s
train loss: 2.2174, val_loss: 3.7284
2022-01-11 10:24:35 | epoch: 0024/100, training time: 13.0s, inference time: 0.6s
train loss: 2.1695, val_loss: 3.6202
2022-01-11 10:24:49 | epoch: 0025/100, training time: 14.0s, inference time: 0.6s
train loss: 2.1291, val_loss: 3.7972
2022-01-11 10:25:04 | epoch: 0026/100, training time: 14.2s, inference time: 0.8s
train loss: 2.0681, val_loss: 3.8992
2022-01-11 10:25:18 | epoch: 0027/100, training time: 13.4s, inference time: 0.7s
train loss: 2.0399, val_loss: 3.7016
2022-01-11 10:25:34 | epoch: 0028/100, training time: 15.1s, inference time: 0.7s
train loss: 2.0068, val_loss: 3.9751
2022-01-11 10:25:48 | epoch: 0029/100, training time: 13.1s, inference time: 0.7s
train loss: 1.9758, val_loss: 3.7833
2022-01-11 10:26:02 | epoch: 0030/100, training time: 13.3s, inference time: 0.6s
train loss: 1.9559, val_loss: 3.8947
2022-01-11 10:26:16 | epoch: 0031/100, training time: 13.3s, inference time: 0.7s
train loss: 1.8529, val_loss: 3.8001
2022-01-11 10:26:30 | epoch: 0032/100, training time: 13.7s, inference time: 0.8s
train loss: 1.8174, val_loss: 3.7532
2022-01-11 10:26:45 | epoch: 0033/100, training time: 13.6s, inference time: 0.7s
train loss: 1.7940, val_loss: 3.9062
2022-01-11 10:26:59 | epoch: 0034/100, training time: 13.4s, inference time: 0.7s
train loss: 1.8101, val_loss: 4.2041
2022-01-11 10:27:14 | epoch: 0035/100, training time: 15.0s, inference time: 0.8s
train loss: 1.7306, val_loss: 3.9790
2022-01-11 10:27:28 | epoch: 0036/100, training time: 13.1s, inference time: 0.6s
train loss: 1.7478, val_loss: 3.9144
2022-01-11 10:27:42 | epoch: 0037/100, training time: 13.1s, inference time: 0.7s
train loss: 1.6715, val_loss: 4.0528
2022-01-11 10:27:56 | epoch: 0038/100, training time: 13.1s, inference time: 0.8s
train loss: 1.6748, val_loss: 3.9580
2022-01-11 10:28:10 | epoch: 0039/100, training time: 13.5s, inference time: 0.6s
train loss: 1.6422, val_loss: 3.9937
2022-01-11 10:28:24 | epoch: 0040/100, training time: 13.4s, inference time: 0.8s
train loss: 1.6252, val_loss: 3.9780
2022-01-11 10:28:40 | epoch: 0041/100, training time: 15.2s, inference time: 0.7s
train loss: 1.5623, val_loss: 4.2681
2022-01-11 10:28:55 | epoch: 0042/100, training time: 14.3s, inference time: 0.7s
train loss: 1.5673, val_loss: 4.0865
2022-01-11 10:29:10 | epoch: 0043/100, training time: 14.1s, inference time: 1.0s
train loss: 1.5471, val_loss: 4.0568
2022-01-11 10:29:28 | epoch: 0044/100, training time: 16.8s, inference time: 0.9s
train loss: 1.5189, val_loss: 4.0250
2022-01-11 10:29:47 | epoch: 0045/100, training time: 18.1s, inference time: 0.9s
train loss: 1.5061, val_loss: 4.1025
2022-01-11 10:30:08 | epoch: 0046/100, training time: 20.4s, inference time: 0.9s
train loss: 1.4821, val_loss: 4.2595
2022-01-11 10:30:26 | epoch: 0047/100, training time: 17.4s, inference time: 0.9s
train loss: 1.4591, val_loss: 4.4524
2022-01-11 10:30:45 | epoch: 0048/100, training time: 17.5s, inference time: 0.9s
train loss: 1.4304, val_loss: 4.1946
2022-01-11 10:31:03 | epoch: 0049/100, training time: 17.7s, inference time: 1.0s
train loss: 1.4151, val_loss: 4.4484
2022-01-11 10:31:22 | epoch: 0050/100, training time: 17.4s, inference time: 0.9s
train loss: 1.4067, val_loss: 4.2261
2022-01-11 10:31:41 | epoch: 0051/100, training time: 18.5s, inference time: 0.9s
train loss: 1.3668, val_loss: 4.4653
2022-01-11 10:31:59 | epoch: 0052/100, training time: 17.3s, inference time: 0.9s
train loss: 1.3510, val_loss: 4.4456
2022-01-11 10:32:18 | epoch: 0053/100, training time: 17.4s, inference time: 1.1s
train loss: 1.3436, val_loss: 4.3060
2022-01-11 10:32:37 | epoch: 0054/100, training time: 18.7s, inference time: 0.9s
train loss: 1.3256, val_loss: 4.5953
2022-01-11 10:33:01 | epoch: 0055/100, training time: 22.4s, inference time: 1.0s
train loss: 1.3309, val_loss: 4.4785
2022-01-11 10:33:21 | epoch: 0056/100, training time: 19.5s, inference time: 0.9s
train loss: 1.3143, val_loss: 4.6830
2022-01-11 10:33:40 | epoch: 0057/100, training time: 17.6s, inference time: 0.9s
train loss: 1.2734, val_loss: 4.3626
2022-01-11 10:33:59 | epoch: 0058/100, training time: 18.1s, inference time: 1.0s
train loss: 1.2645, val_loss: 4.4257
2022-01-11 10:34:18 | epoch: 0059/100, training time: 18.0s, inference time: 0.9s
train loss: 1.2911, val_loss: 4.3821
2022-01-11 10:34:37 | epoch: 0060/100, training time: 18.2s, inference time: 1.0s
train loss: 1.2910, val_loss: 4.4105
2022-01-11 10:34:56 | epoch: 0061/100, training time: 18.1s, inference time: 0.9s
train loss: 1.2350, val_loss: 4.3423
2022-01-11 10:35:15 | epoch: 0062/100, training time: 17.6s, inference time: 1.2s
train loss: 1.2115, val_loss: 4.7396
2022-01-11 10:35:34 | epoch: 0063/100, training time: 18.0s, inference time: 0.9s
train loss: 1.1854, val_loss: 4.5727
2022-01-11 10:35:53 | epoch: 0064/100, training time: 18.0s, inference time: 0.9s
train loss: 1.1840, val_loss: 4.5012
2022-01-11 10:36:12 | epoch: 0065/100, training time: 18.1s, inference time: 0.9s
train loss: 1.1497, val_loss: 4.7457
2022-01-11 10:36:30 | epoch: 0066/100, training time: 17.6s, inference time: 0.9s
train loss: 1.1856, val_loss: 4.8076
2022-01-11 10:36:49 | epoch: 0067/100, training time: 17.4s, inference time: 0.9s
train loss: 1.2014, val_loss: 4.5112
2022-01-11 10:37:07 | epoch: 0068/100, training time: 17.5s, inference time: 0.9s
train loss: 1.1725, val_loss: 4.6171
2022-01-11 10:37:25 | epoch: 0069/100, training time: 17.4s, inference time: 0.9s
train loss: 1.1332, val_loss: 4.6223
2022-01-11 10:37:44 | epoch: 0070/100, training time: 18.2s, inference time: 1.0s
train loss: 1.1069, val_loss: 4.6218
2022-01-11 10:38:04 | epoch: 0071/100, training time: 18.2s, inference time: 1.0s
train loss: 1.1136, val_loss: 4.6271
2022-01-11 10:38:22 | epoch: 0072/100, training time: 17.3s, inference time: 0.8s
train loss: 1.0810, val_loss: 4.6362
2022-01-11 10:38:40 | epoch: 0073/100, training time: 17.1s, inference time: 0.9s
train loss: 1.0681, val_loss: 4.5136
2022-01-11 10:38:57 | epoch: 0074/100, training time: 16.8s, inference time: 0.9s
train loss: 1.1037, val_loss: 4.5000
2022-01-11 10:39:15 | epoch: 0075/100, training time: 16.8s, inference time: 0.9s
train loss: 1.0817, val_loss: 4.6121
2022-01-11 10:39:33 | epoch: 0076/100, training time: 16.9s, inference time: 0.9s
train loss: 1.0595, val_loss: 4.5290
2022-01-11 10:39:52 | epoch: 0077/100, training time: 18.3s, inference time: 0.9s
train loss: 1.0831, val_loss: 4.5394
2022-01-11 10:40:11 | epoch: 0078/100, training time: 17.7s, inference time: 0.9s
train loss: 1.0549, val_loss: 4.4662
2022-01-11 10:40:31 | epoch: 0079/100, training time: 19.6s, inference time: 1.0s
train loss: 1.0299, val_loss: 4.8361
2022-01-11 10:40:51 | epoch: 0080/100, training time: 19.1s, inference time: 0.9s
train loss: 1.0411, val_loss: 4.5626
2022-01-11 10:41:10 | epoch: 0081/100, training time: 17.7s, inference time: 0.9s
train loss: 0.9880, val_loss: 4.7100
2022-01-11 10:41:28 | epoch: 0082/100, training time: 17.6s, inference time: 0.9s
train loss: 1.0209, val_loss: 4.6126
2022-01-11 10:41:47 | epoch: 0083/100, training time: 18.0s, inference time: 0.9s
train loss: 1.0137, val_loss: 4.8191
2022-01-11 10:42:07 | epoch: 0084/100, training time: 18.7s, inference time: 1.0s
train loss: 0.9813, val_loss: 4.6453
2022-01-11 10:42:27 | epoch: 0085/100, training time: 19.3s, inference time: 0.8s
train loss: 0.9920, val_loss: 4.6193
2022-01-11 10:42:49 | epoch: 0086/100, training time: 21.3s, inference time: 1.0s
train loss: 0.9711, val_loss: 4.6748
2022-01-11 10:43:10 | epoch: 0087/100, training time: 19.3s, inference time: 0.9s
train loss: 0.9529, val_loss: 4.6272
2022-01-11 10:43:29 | epoch: 0088/100, training time: 18.5s, inference time: 0.9s
train loss: 0.9510, val_loss: 4.8605
2022-01-11 10:43:51 | epoch: 0089/100, training time: 21.0s, inference time: 1.1s
train loss: 0.9477, val_loss: 4.5917
2022-01-11 10:44:11 | epoch: 0090/100, training time: 18.9s, inference time: 0.9s
train loss: 0.9738, val_loss: 4.8606
2022-01-11 10:44:31 | epoch: 0091/100, training time: 18.9s, inference time: 0.9s
train loss: 0.9284, val_loss: 4.8032
2022-01-11 10:44:50 | epoch: 0092/100, training time: 18.1s, inference time: 0.9s
train loss: 0.9230, val_loss: 4.7853
2022-01-11 10:45:08 | epoch: 0093/100, training time: 17.7s, inference time: 0.9s
train loss: 0.9252, val_loss: 4.7353
2022-01-11 10:45:27 | epoch: 0094/100, training time: 17.7s, inference time: 0.9s
train loss: 0.9765, val_loss: 4.7886
2022-01-11 10:45:46 | epoch: 0095/100, training time: 17.9s, inference time: 0.9s
train loss: 0.9498, val_loss: 4.9179
2022-01-11 10:46:04 | epoch: 0096/100, training time: 17.6s, inference time: 0.9s
train loss: 0.9514, val_loss: 4.6917
2022-01-11 10:46:25 | epoch: 0097/100, training time: 19.9s, inference time: 0.9s
train loss: 0.9171, val_loss: 4.7151
2022-01-11 10:46:44 | epoch: 0098/100, training time: 17.8s, inference time: 1.0s
train loss: 0.8987, val_loss: 4.7659
2022-01-11 10:47:04 | epoch: 0099/100, training time: 18.9s, inference time: 0.9s
train loss: 0.9294, val_loss: 4.7392
2022-01-11 10:47:22 | epoch: 0100/100, training time: 17.2s, inference time: 0.9s
train loss: 0.9090, val_loss: 4.6176
Training and validation are completed, and model has been stored as ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
**** testing model ****
loading model from ./output/hyperparameters_experiment__gpu_lr0.001/model.pkl
model restored!
evaluating...
testing time: 1.8s
                MAE		RMSE		MAPE
train            0.54		0.81		8.30%
val              1.36		2.18		20.62%
test             1.21		2.05		19.26%
performance in each prediction step
step: 01         1.21		2.05		19.26%
average:         1.21		2.05		19.26%
total time: 29.0min
